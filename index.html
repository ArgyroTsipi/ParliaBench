<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ParliaBench: LLM-Generated Parliamentary Speech Evaluation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            margin-bottom: 40px;
            border-radius: 8px;
        }

        h1 {
            font-size: 2.2em;
            margin-bottom: 10px;
            line-height: 1.3;
        }

        h2 {
            color: #667eea;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }

        h3 {
            color: #764ba2;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        h4 {
            color: #555;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        /* Tab Navigation */
        .tab-navigation {
            display: flex;
            gap: 10px;
            margin-bottom: 30px;
            background: white;
            padding: 10px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow-x: auto;
        }

        .tab-button {
            padding: 15px 25px;
            background: #f0f0f0;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-size: 1em;
            font-weight: 500;
            transition: all 0.3s ease;
            white-space: nowrap;
            flex-shrink: 0;
        }

        .tab-button:hover {
            background: #e0e0e0;
            transform: translateY(-2px);
        }

        .tab-button.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .card {
            background: white;
            padding: 30px;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .stat-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .stat-number {
            font-size: 2em;
            font-weight: bold;
            margin-bottom: 5px;
        }

        .stat-label {
            font-size: 0.9em;
            opacity: 0.9;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: #f5f5f5;
        }

        .pipeline {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .pipeline-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-left: 4px solid #667eea;
            border-radius: 4px;
        }

        .highlight {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #ffc107;
        }

        ul {
            margin-left: 20px;
            margin-top: 10px;
        }

        li {
            margin: 8px 0;
        }

        .footer {
            text-align: center;
            padding: 20px;
            color: #666;
            margin-top: 40px;
        }

        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            .tab-button {
                padding: 12px 20px;
                font-size: 0.9em;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech</h1>
            <p>UK Parliamentary Proceedings Dataset (2015-2022)</p>
        </header>

        <div class="card" style="background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);">
            <h2 style="color: #333; border-bottom: none; margin-top: 0;">Project Overview</h2>
            <p style="font-size: 1.1em; line-height: 1.8;">
                This project explores political dialogue generation using large language models fine-tuned on UK parliamentary speeches. 
                It encompasses data processing, model selection, fine-tuning with QLoRA, speech generation, and comprehensive evaluation 
                across linguistic, semantic, and political dimensions.
            </p>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-top: 20px;">
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>üìä Data Processing</strong><br>
                    <small>447K speeches, 1.9K speakers</small>
                </div>
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>ü§ñ Models</strong><br>
                    <small>5 LLMs with QLoRA</small>
                </div>
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>üí¨ Generation</strong><br>
                    <small>2.7K speeches per model</small>
                </div>
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>üìà Evaluation</strong><br>
                    <small>12+ metrics across 4 dimensions</small>
                </div>
            </div>
        </div>

        <!-- Tab Navigation -->
        <div class="tab-navigation">
            <button class="tab-button active" onclick="openTab(event, 'data-processing')">üìä Data Processing</button>
            <button class="tab-button" onclick="openTab(event, 'model-training')">ü§ñ Model Training & Fine-tuning</button>
            <button class="tab-button" onclick="openTab(event, 'speech-generation')">üí¨ Speech Generation</button>
            <button class="tab-button" onclick="openTab(event, 'evaluation-metrics')">üìè Evaluation Metrics</button>
            <button class="tab-button" onclick="openTab(event, 'results')">üìà Results</button>
        </div>

        <!-- Tab 1: Data Processing -->
        <div id="data-processing" class="tab-content active">
            <div class="card">
                <h2>Dataset Overview</h2>
                <p>The ParlaMint-GB dataset version 5.0 from CLARIN contains structured UK parliamentary proceedings with comprehensive metadata including speaker information, political affiliations, gender, and complete speech transcripts.</p>
                
                <div class="stats-grid">
                    <div class="stat-box">
                        <div class="stat-number">447,778</div>
                        <div class="stat-label">Total Speeches</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-number">1,901</div>
                        <div class="stat-label">Unique Speakers</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-number">11</div>
                        <div class="stat-label">Political Parties</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-number">~99.94M</div>
                        <div class="stat-label">Total Words</div>
                    </div>
                </div>

                <div class="highlight">
                    <strong>Time Period:</strong> January 5, 2015 to July 21, 2022<br>
                    <strong>Houses:</strong> House of Commons & House of Lords<br>
                    <strong>Mean Words per Speech:</strong> 223.2 | <strong>Median:</strong> 99.0
                </div>
            </div>

            <div class="card">
                <h2>Data Cleaning Criteria</h2>
                <ul>
                    <li>Kept only parties with more than 1,000 speeches</li>
                    <li>Removed speeches with less than 35 words (5th percentile)</li>
                    <li>Removed speeches with over 1,580 words (99th percentile)</li>
                    <li>Filtered out "Unknown" party affiliation</li>
                    <li>Removed "Business of the House" and "Point of Order" sections</li>
                    <li>Standardized quotation marks to regular double quotes</li>
                </ul>
            </div>

            <div class="card">
                <h2>Party Distribution</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Party</th>
                            <th>Political Orientation</th>
                            <th>Total Speeches</th>
                            <th>Percentage</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Conservative</td>
                            <td>Centre-right</td>
                            <td>263,513</td>
                            <td>58.85%</td>
                        </tr>
                        <tr>
                            <td>Labour</td>
                            <td>Centre-left</td>
                            <td>108,831</td>
                            <td>24.31%</td>
                        </tr>
                        <tr>
                            <td>Scottish National Party</td>
                            <td>Centre-left</td>
                            <td>23,562</td>
                            <td>5.26%</td>
                        </tr>
                        <tr>
                            <td>Liberal Democrats</td>
                            <td>Centre to centre-left</td>
                            <td>23,517</td>
                            <td>5.25%</td>
                        </tr>
                        <tr>
                            <td>Crossbench</td>
                            <td>Unknown</td>
                            <td>11,878</td>
                            <td>2.65%</td>
                        </tr>
                        <tr>
                            <td>Democratic Unionist Party</td>
                            <td>Right</td>
                            <td>6,610</td>
                            <td>1.48%</td>
                        </tr>
                        <tr>
                            <td>Others</td>
                            <td>Various</td>
                            <td>9,867</td>
                            <td>2.20%</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="card">
                <h2>Data Processing Pipeline</h2>
                <div class="pipeline">
                    <div class="pipeline-step">
                        <h3>1. XML Parsing & Metadata Extraction</h3>
                        <ul>
                            <li>Parse speaker information from <code>listPerson.xml</code></li>
                            <li>Extract political affiliations with temporal bounds</li>
                            <li>Extract speech content from dated session XML files</li>
                            <li>Filter out procedural elements</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h3>2. Temporal Alignment</h3>
                        <ul>
                            <li>Match speeches to correct political party at time of delivery</li>
                            <li>Handle party changes and role transitions</li>
                            <li>Use temporal validity ranges (<code>@from</code> and <code>@to</code> attributes)</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h3>3. Prompt Extraction</h3>
                        <ul>
                            <li>Identify and separate question prompts from speeches</li>
                            <li>Store prompts as list of strings</li>
                            <li>Clean prompts by removing number and letter prefixes</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h3>4. Political Orientation Classification</h3>
                        <ul>
                            <li>Extract political orientation codes from <code>ParlaMint-listOrg.xml</code></li>
                            <li>Map codes to orientation labels (Left, Centre, Right, etc.)</li>
                            <li>13 distinct orientation categories</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h3>5. Topic Categorization (EuroVoc)</h3>
                        <ul>
                            <li>Direct mapping for 13 categories with clear CAP-EuroVoc correspondence</li>
                            <li>Automated classification using KEVLar for complex categories</li>
                            <li>21 EuroVoc thematic categories</li>
                            <li>Highest confidence score used for final topic assignment</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>EuroVoc Topic Categories</h2>
                <p>Speeches were classified into 21 thematic categories using the EuroVoc taxonomy:</p>
                <div style="columns: 2; column-gap: 30px; margin-top: 20px;">
                    <ul>
                        <li>International Relations</li>
                        <li>Law</li>
                        <li>Social Questions</li>
                        <li>Politics</li>
                        <li>Education and Communications</li>
                        <li>Geography</li>
                        <li>Economics</li>
                        <li>Employment and Working Conditions</li>
                        <li>European Union</li>
                        <li>Transport</li>
                        <li>Trade</li>
                        <li>Environment</li>
                        <li>Production, Technology and Research</li>
                        <li>Energy</li>
                        <li>Agriculture, Forestry and Fisheries</li>
                        <li>Finance</li>
                        <li>Industry</li>
                        <li>Business and Competition</li>
                        <li>Agri-foodstuffs</li>
                        <li>International Organisations</li>
                        <li>Science</li>
                    </ul>
                </div>
            </div>

            <div class="card">
                <h2>Training Data Structure</h2>
                <p>Each training instance contains the following features:</p>
                <ul>
                    <li><strong>speech:</strong> The parliamentary speech text</li>
                    <li><strong>section:</strong> Debate section context</li>
                    <li><strong>party:</strong> Political party affiliation</li>
                    <li><strong>prompts:</strong> Associated question prompts</li>
                    <li><strong>house:</strong> House of Commons or House of Lords</li>
                    <li><strong>political_orientation_label:</strong> Political orientation classification</li>
                    <li><strong>eurovoc_topic:</strong> Thematic category</li>
                </ul>

                <div class="highlight" style="margin-top: 20px;">
                    <strong>Train-Test Split:</strong> 80% training / 20% test (random seed: 42)
                </div>
            </div>

            <div class="card">
                <h2>Key Statistics</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Statistic</th>
                            <th>Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Mean words per speech</td>
                            <td>223.2</td>
                        </tr>
                        <tr>
                            <td>Median words per speech</td>
                            <td>99.0</td>
                        </tr>
                        <tr>
                            <td>Standard deviation</td>
                            <td>278.7</td>
                        </tr>
                        <tr>
                            <td>Minimum words</td>
                            <td>36</td>
                        </tr>
                        <tr>
                            <td>Maximum words</td>
                            <td>1,579</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <!-- Tab 2: Model Training & Fine-tuning -->
        <div id="model-training" class="tab-content">
            <div class="card">
                <h2>Model Selection</h2>
                <p>Five large language models were selected based on their architecture, performance, and compatibility with the UNSLOTH fine-tuning framework. All models use 4-bit quantization for memory efficiency.</p>
                
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Memory Reduction</th>
                            <th>Inference Speed</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>mistral-7b-v0.3-bnb-4bit</td>
                            <td>62%</td>
                            <td>2.2√ó</td>
                        </tr>
                        <tr>
                            <td>Meta-Llama-3.1-8B-bnb-4bit</td>
                            <td>58%</td>
                            <td>2.4√ó</td>
                        </tr>
                        <tr>
                            <td>gemma-2-9b-bnb-4bit </td>
                            <td>58%</td>
                            <td>2.2√ó</td>
                        </tr>
                        <tr>
                            <td>Qwen2-7B-bnb-4bit </td>
                            <td>N/A</td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td>Yi-1.5-6b-bnb-4bit</td>
                            <td>N/A</td>
                            <td>N/A</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="card">
                <h2>Fine-Tuning Methodology</h2>
                
                <h3>QLoRA (Quantized Low-Rank Adaptation)</h3>
                <p>Parameter-efficient fine-tuning using 4-bit quantization with low-rank matrix adaptation, enabling efficient model customization without massive computational resources.</p>

                <div class="pipeline">
                    <div class="pipeline-step">
                        <h3>QLoRA Configuration</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Parameter</th>
                                    <th>Value</th>
                                    <th>Rationale</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>LoRA Rank (r)</td>
                                    <td>16</td>
                                    <td>Optimal balance for fast fine-tuning</td>
                                </tr>
                                <tr>
                                    <td>LoRA Alpha</td>
                                    <td>16</td>
                                    <td>Set equal to rank (Œ±/r = 1) for baseline</td>
                                </tr>
                                <tr>
                                    <td>Target Modules</td>
                                    <td>7 layers</td>
                                    <td>All linear transformations</td>
                                </tr>
                                <tr>
                                    <td>LoRA Dropout</td>
                                    <td>0</td>
                                    <td>Enable Unsloth optimizations</td>
                                </tr>
                                <tr>
                                    <td>Bias Configuration</td>
                                    <td>none</td>
                                    <td>Faster training, reduced memory</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="pipeline-step">
                        <h3>Training Configuration</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Parameter</th>
                                    <th>Value</th>
                                    <th>Justification</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Batch Size</td>
                                    <td>64</td>
                                    <td>GPU memory optimization</td>
                                </tr>
                                <tr>
                                    <td>Learning Rate</td>
                                    <td>2e-4</td>
                                    <td>Standard for LoRA fine-tuning</td>
                                </tr>
                                <tr>
                                    <td>Max Steps</td>
                                    <td>11,194</td>
                                    <td>2 epochs</td>
                                </tr>
                                <tr>
                                    <td>Warmup Steps</td>
                                    <td>336</td>
                                    <td>10% of max steps for stability</td>
                                </tr>
                                <tr>
                                    <td>Optimizer</td>
                                    <td>AdamW</td>
                                    <td>Memory-efficient</td>
                                </tr>
                                <tr>
                                    <td>Weight Decay</td>
                                    <td>0.01</td>
                                    <td>Prevents overfitting on political data</td>
                                </tr>
                                <tr>
                                    <td>Max Sequence Length</td>
                                    <td>1024</td>
                                    <td>Optimal for dataset median length</td>
                                </tr>
                                <tr>
                                    <td>Scheduler</td>
                                    <td>Linear</td>
                                    <td>Linear learning rate schedule</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="pipeline-step">
                        <h3>System Prompt</h3>
                        <div style="background: #f8f9fa; padding: 15px; border-radius: 4px; font-family: monospace; margin-top: 10px;">
                            You are a seasoned UK parliamentary member. Use proper British parliamentary language appropriate for the specified House. The speech should reflect the political orientation and typical positions of the specified party on the given topic.
                        </div>
                    </div>

                    <div class="pipeline-step">
                        <h3>Context Fields (Input to Model)</h3>
                        <ul>
                            <li><strong>PARTY:</strong> Political party affiliation (e.g., Conservative)</li>
                            <li><strong>EUROVOC TOPIC:</strong> Thematic classification (e.g., TRADE)</li>
                            <li><strong>SECTION:</strong> Parliamentary debate section</li>
                            <li><strong>POLITICAL ORIENTATION:</strong> Orientation label (e.g., Centre-right)</li>
                            <li><strong>HOUSE:</strong> House of Commons or House of Lords</li>
                            <li><strong>INSTRUCTION:</strong> Question prompt or generic instruction</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>Tools & Environment</h2>
                <div class="stats-grid">
                    <div class="stat-box">
                        <div class="stat-label">Framework</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">Unsloth + HuggingFace</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-label">Backend</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">PyTorch</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-label">Hardware</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">AWS A100 GPU</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-label">Training Method</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">Supervised Fine-Tuning</div>
                    </div>
                </div>

        </div>

        <!-- Tab 3: Speech Generation -->
        <div id="speech-generation" class="tab-content">
            <div class="card">
                <h2>Speech Generation Pipeline</h2>
                <p>A systematic speech generation system that loads trained models and creates political speeches based on structured inputs.</p>

                <h3>Input Distribution</h3>
                <p>All models received identical generation tasks with realistic distributional characteristics:</p>
                
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                    <div>
                        <h4>House Distribution</h4>
                        <ul>
                            <li>House of Commons: 78%</li>
                            <li>House of Lords: 22%</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Top Parties by Weight</h4>
                        <ul>
                            <li>Conservative: 59%</li>
                            <li>Labour: 24%</li>
                            <li>Scottish National Party: 5%</li>
                            <li>Liberal Democrats: 5%</li>
                        </ul>
                    </div>
                </div>

                <h3>Generation Parameters</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Parameter</th>
                            <th>Value</th>
                            <th>Purpose</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Speeches per Model</td>
                            <td>2,700</td>
                            <td>Comprehensive evaluation dataset</td>
                        </tr>
                        <tr>
                            <td>Temperature</td>
                            <td>0.7</td>
                            <td>Balances coherence and variation</td>
                        </tr>
                        <tr>
                            <td>Top-p (Nucleus Sampling)</td>
                            <td>0.85</td>
                            <td>Focused yet diverse outputs</td>
                        </tr>
                        <tr>
                            <td>Repetition Penalty</td>
                            <td>1.2</td>
                            <td>Prevents redundant phrasing</td>
                        </tr>
                        <tr>
                            <td>Batch Size</td>
                            <td>32</td>
                            <td>3√ó speed improvement</td>
                        </tr>
                        <tr>
                            <td>Min Word Count</td>
                            <td>43</td>
                            <td>P10 threshold for quality</td>
                        </tr>
                        <tr>
                            <td>Max Word Count</td>
                            <td>635</td>
                            <td>P90 threshold for quality</td>
                        </tr>
                        <tr>
                            <td>Max New Tokens</td>
                            <td>850</td>
                            <td>1.33√ó P90 speech length</td>
                        </tr>
                    </tbody>
                </table>

                <div class="highlight" style="margin-top: 20px;">
                    <strong>Decoding Strategy:</strong> Nucleus sampling (top-p) chosen over greedy/beam search to avoid repetitive or incoherent text
                </div>
            </div>

            <div class="card">
                <h2>Speech Validation Process</h2>
                <p>9-step validation procedure to ensure quality, coherence, and relevance of generated speeches:</p>
                
                <div class="pipeline">
                    <div class="pipeline-step">
                        <strong>1. Template Marker Detection</strong><br>
                        Detects 27 template artifacts (role markers, context labels, special tokens)
                    </div>
                    <div class="pipeline-step">
                        <strong>2. Unicode Corruption Detection</strong><br>
                        Identifies 14 corruption patterns and checks 11 forbidden Unicode ranges (CJK, Cyrillic, Arabic, etc.)
                    </div>
                    <div class="pipeline-step">
                        <strong>3. Language Detection</strong><br>
                        Uses spacy-langdetect to flag non-English text (&gt;85% confidence threshold)
                    </div>
                    <div class="pipeline-step">
                        <strong>4. Repetition Detection</strong><br>
                        Three patterns: (1) Same word &gt;3 times, (2) Sequences of 3-7 words &gt;3 times, (3) Counting patterns
                    </div>
                    <div class="pipeline-step">
                        <strong>5. Semantic Relevance Check</strong><br>
                        Cosine similarity between speech and context (threshold: &lt;0.08 flagged as off-topic)
                    </div>
                    <div class="pipeline-step">
                        <strong>6. Length Constraints</strong><br>
                        Validates word count (43-635 words)
                    </div>
                    <div class="pipeline-step">
                        <strong>7. Concatenation Detection</strong><br>
                        Detects multiple opening phrases (‚â•4 instances of "My Lords", "Mr Speaker", etc.)
                    </div>
                    <div class="pipeline-step">
                        <strong>8. Corrupted Endings Detection</strong><br>
                        Identifies nonsensical endings
                    </div>
                    <div class="pipeline-step">
                        <strong>9. Refusal Pattern Matching</strong><br>
                        Catches AI refusal patterns ("I cannot generate", "I'm sorry but...")
                    </div>
                </div>
            </div>
        </div>

        <!-- Tab 4: Evaluation Metrics -->
        <div id="evaluation-metrics" class="tab-content">
            <div class="card">
                <h2>Evaluation Framework</h2>
                <p>Comprehensive multi-dimensional assessment system evaluating linguistic quality, semantic coherence, political alignment, and overall effectiveness.</p>

                <h3>1. Linguistic Quality & Diversity Metrics</h3>
                
                <div class="pipeline">
                    <div class="pipeline-step">
                        <h4>Perplexity</h4>
                        <p>Measures how natural the text appears to a language model. Lower scores indicate more human-like, predictable text.</p>
                        <ul>
                            <li><strong>Model:</strong> GPT-2 base (117M parameters)</li>
                            <li><strong>Processing:</strong> Max 512 words per speech, batch size 8</li>
                            <li><strong>Interpretation:</strong> Lower = more natural</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h4>Distinct-N (N=1,2,3,4)</h4>
                        <p>Evaluates lexical diversity by measuring ratio of unique n-grams to total tokens.</p>
                        <ul>
                            <li><strong>Distinct-1:</strong> Unique unigrams (basic lexical diversity)</li>
                            <li><strong>Distinct-2:</strong> Unique bigrams (phrase-level variety)</li>
                            <li><strong>Distinct-3/4:</strong> Multi-word patterns (sophisticated language use)</li>
                            <li><strong>Interpretation:</strong> Higher = more diverse vocabulary</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h4>Self-BLEU</h4>
                        <p>Measures similarity between generated texts from the same model to detect repetitive content.</p>
                        <ul>
                            <li><strong>Method:</strong> Each speech compared to all others from same model</li>
                            <li><strong>Interpretation:</strong> Lower = higher diversity (desirable)</li>
                        </ul>
                    </div>
                </div>

                <h3>2. Semantic Coherence & Text Quality</h3>
                
                <div class="pipeline">
                    <div class="pipeline-step">
                        <h4>GRUEN Score</h4>
                        <p>Comprehensive quality metric combining Grammaticality, non-Redundancy, focUs, structurE, and coNherence.</p>
                        <ul>
                            <li><strong>Grammaticality:</strong> BERT perplexity + CoLA classifier (0-1 scale)</li>
                            <li><strong>Non-Redundancy:</strong> LCS, Edit Distance, Word Overlap between sentences</li>
                            <li><strong>Focus:</strong> Word Mover's Distance + SpaCy semantic similarity</li>
                            <li><strong>Formula:</strong> GRUEN = min(1, max(0, G + R_penalty + F_penalty))</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h4>BERTScore</h4>
                        <p>Measures semantic similarity between generated and real speeches using contextualized embeddings.</p>
                        <ul>
                            <li><strong>Model:</strong> RoBERTa-large (auto-selected for English)</li>
                            <li><strong>References:</strong> N=6 most semantically similar speeches from ParlaMint-GB</li>
                            <li><strong>Metrics:</strong> Precision, Recall, F1</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h4>MoverScore</h4>
                        <p>Computes optimal transport cost between embedding distributions using Earth Mover's Distance.</p>
                        <ul>
                            <li><strong>Model:</strong> DistilBERT-base-uncased</li>
                            <li><strong>Method:</strong> IDF-weighted embeddings with POT library</li>
                            <li><strong>References:</strong> N=6 most semantically similar speeches from ParlaMint-GB</li>
                            <li><strong>Score Range:</strong> 0-1 (higher = better alignment)</li>
                        </ul>
                    </div>
                </div>

                <h3>3. Political Alignment Metrics</h3>
                
                <div class="pipeline">
                    <div class="pipeline-step">
                        <h4>Political Spectrum Alignment (PSA)</h4>
                        <p>Measures ideological alignment with expected political orientation (13-point scale from Far-left to Far-right).</p>
                        <ul>
                            <li><strong>Model:</strong> all-mpnet-base-v2 sentence transformer</li>
                            <li><strong>Method:</strong> Cosine similarity to orientation centroids</li>
                            <li><strong>Formula:</strong> PSA = cosine_similarity √ó max(0, 100 - d/12 √ó 100)</li>
                            <li><strong>Range:</strong> 0-1 (higher = better alignment)</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <h4>Party Alignment</h4>
                        <p>Assesses whether speech captures party-specific linguistic characteristics.</p>
                        <ul>
                            <li><strong>Model:</strong> all-mpnet-base-v2 sentence transformer</li>
                            <li><strong>Method:</strong> Cosine similarity to party-specific centroids</li>
                            <li><strong>Range:</strong> 0-1 (higher = better party alignment)</li>
                        </ul>
                    </div>
                </div>

                <h3>4. LLM-as-a-Judge Evaluation</h3>
                <p>Automated assessment using Flow-Judge-v0.1 (3.8B parameters, 4-bit quantization) across six dimensions on a 10-point scale with rubrics for each metric:</p>

                <table style="margin-top: 20px;">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Evaluation Criteria</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Coherence</strong></td>
                            <td>Logical flow, argument connectivity, parliamentary structure</td>
                        </tr>
                        <tr>
                            <td><strong>Conciseness</strong></td>
                            <td>Efficient message delivery without excessive verbosity (parliamentary context)</td>
                        </tr>
                        <tr>
                            <td><strong>Relevance</strong></td>
                            <td>Direct addressing of prompt/question with complete coverage</td>
                        </tr>
                        <tr>
                            <td><strong>Authenticity</strong></td>
                            <td>Natural Westminster discourse vs AI-generated patterns</td>
                        </tr>
                        <tr>
                            <td><strong>Political Appropriateness</strong></td>
                            <td>Alignment with party's typical positions and rhetoric</td>
                        </tr>
                        <tr>
                            <td><strong>Overall Quality</strong></td>
                            <td>Effectiveness as political communication, argumentation sophistication</td>
                        </tr>
                    </tbody>
                </table>

                <div class="highlight" style="margin-top: 20px;">
                    <strong>Configuration:</strong> Batch size 32, Temperature 0.3, Max new tokens 2000, Default score -1 for errors
                </div>
            </div>

            <div class="card">
                <h2>Evaluation Summary</h2>
                <div class="stats-grid">
                    <div class="stat-box">
                        <div class="stat-label">Linguistic Metrics</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">Perplexity, Distinct-N, Self-BLEU</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-label">Semantic Metrics</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">GRUEN, BERTScore, MoverScore</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-label">Political Metrics</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">PSA, Party Alignment</div>
                    </div>
                    <div class="stat-box">
                        <div class="stat-label">Judge Metrics</div>
                        <div style="font-size: 1.2em; margin-top: 10px;">6 Dimensions (1-10 scale)</div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Tab 5: Results -->
        <div id="results" class="tab-content">
            <div class="card" style="background: linear-gradient(135deg, #e0f7fa 0%, #80deea 100%);">
                <h2 style="color: #006064; border-bottom: none; margin-top: 0;">Key Findings & Results</h2>
                
                <div class="highlight" style="background: white; border-left: 4px solid #00796b; margin-bottom: 30px;">
                    <strong style="color: #00796b;">Research Objective:</strong> Investigate the generation of authentic political discourse through domain-specific fine-tuning of large language models on UK parliamentary speeches.
                </div>

                <h3 style="color: #00796b;">1. Architectural Design & Context Window Effects</h3>
                <div style="background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px;">
                    <p><strong>Best Overall Performer: Llama 3.1 8B</strong></p>
                    <ul>
                        <li>‚úÖ <strong>128,000-token context window</strong> enabled superior performance across multiple dimensions</li>
                        <li>‚úÖ Enhanced instruction-following capabilities</li>
                        <li>‚úÖ Better captured argumentative structure and rhetorical patterns</li>
                        <li>‚úÖ Successfully referenced prior statements and built cumulative cases</li>
                    </ul>

                    <p style="margin-top: 20px;"><strong>Most Stable Performer: Gemma 2 9B</strong></p>
                    <ul>
                        <li>‚úÖ Consistent scores across all political parties</li>
                        <li>‚úÖ Performed well regardless of party affiliation or training data abundance</li>
                        <li>‚úÖ Strong cross-context stability</li>
                    </ul>

                    <p style="margin-top: 20px;"><strong>Second Best: Yi 1.5 6B</strong></p>
                    <ul>
                        <li>‚úÖ Outperformed larger models despite fewer parameters</li>
                        <li>‚úÖ Bilingual pretraining advantage</li>
                        <li>‚úÖ 3-trillion-token corpus exposure conferred generalization benefits</li>
                    </ul>

                    <p style="margin-top: 20px;"><strong>Weakest Performer: Mistral 7B v0.3</strong></p>
                    <ul>
                        <li>‚ùå Scored below 0.50 on technical topics (Science: 0.483, Agri-foodstuffs: 0.475)</li>
                        <li>‚ùå Struggled with ideologically diverse parties (Non-Affiliated: 0.436, Independent: 0.482)</li>
                        <li>‚ùå 8,000-token sliding window insufficient for extended contextual dependencies</li>
                        <li>‚ùå Poor at capturing nuanced ideological positioning</li>
                    </ul>
                </div>

                <h3 style="color: #00796b;">2. Domain-Specific Fine-Tuning Impact</h3>
                <div style="background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px;">
                    <div class="stats-grid" style="margin-bottom: 20px;">
                        <div class="stat-box" style="background: linear-gradient(135deg, #43a047 0%, #66bb6a 100%);">
                            <div class="stat-number">45/70</div>
                            <div class="stat-label">Metrics with Significant Improvement</div>
                        </div>
                        <div class="stat-box" style="background: linear-gradient(135deg, #1e88e5 0%, #42a5f5 100%);">
                            <div class="stat-number">64%</div>
                            <div class="stat-label">Success Rate</div>
                        </div>
                    </div>

                    <p><strong>Statistically Robust Findings (p < 0.05):</strong></p>
                    <ul>
                        <li>‚úÖ <strong>Self-BLEU decreased:</strong> Reduced formulaic repetition in favor of contextually appropriate variation</li>
                        <li>‚úÖ <strong>Political Spectrum Alignment (PSA) improved significantly:</strong> Better captured ideological positioning of different political orientations</li>
                        <li>‚úÖ <strong>Party Alignment scores increased:</strong> Enhanced fidelity to party-specific rhetoric, policy positions, and argumentative strategies</li>
                        <li>‚úÖ <strong>Improvements held across all model architectures:</strong> Domain adaptation through supervised fine-tuning is transferable and reliable</li>
                    </ul>

                    <div class="highlight" style="margin-top: 20px;">
                        <strong>Key Insight:</strong> Fine-tuning improvements were not uniformly distributed‚Äîpolitical authenticity metrics showed the strongest gains, demonstrating that domain adaptation is particularly effective for capturing ideological authenticity.
                    </div>
                </div>

                <h3 style="color: #00796b;">3. Novel Political Authenticity Metrics Validation</h3>
                <div style="background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px;">
                    <p><strong>Methodological Contribution:</strong> Introduction and validation of Political Spectrum Alignment (PSA) and Party Alignment metrics extends beyond conventional NLP evaluation approaches.</p>

                    <div class="stats-grid" style="margin: 20px 0;">
                        <div class="stat-box" style="background: linear-gradient(135deg, #8e24aa 0%, #ab47bc 100%);">
                            <div style="font-size: 1.5em; margin-bottom: 10px;">p < 0.001</div>
                            <div class="stat-label">Statistical Significance</div>
                        </div>
                        <div class="stat-box" style="background: linear-gradient(135deg, #5e35b1 0%, #7e57c2 100%);">
                            <div style="font-size: 1.2em; margin-bottom: 10px;">High Confidence</div>
                            <div class="stat-label">Discriminative Testing</div>
                        </div>
                    </div>

                    <p><strong>Validation Results:</strong></p>
                    <ul>
                        <li>‚úÖ <strong>Party Alignment:</strong> Successfully discriminates between parties, achieving differentiation even for ideologically proximate parties (e.g., Labour vs. Liberal Democrats)</li>
                        <li>‚úÖ <strong>PSA:</strong> Successfully distinguishes political orientations across the left-right spectrum</li>
                        <li>‚úÖ Both metrics capture their intended political dimensions with high statistical confidence</li>
                    </ul>

                    <div class="highlight" style="margin-top: 20px;">
                        <strong>Impact:</strong> These metrics provide a validated framework for evaluating political authenticity in generated text, applicable to future research in political discourse generation.
                    </div>
                </div>

                <h3 style="color: #00796b;">Summary of Principal Findings</h3>
                <div style="background: white; padding: 20px; border-radius: 8px;">
                    <ol style="line-height: 2;">
                        <li><strong>Architecture matters:</strong> Extended context windows and advanced attention mechanisms are crucial for capturing complex political discourse</li>
                        <li><strong>Fine-tuning works:</strong> Domain-specific adaptation significantly improves political authenticity across diverse model architectures</li>
                        <li><strong>Novel metrics validated:</strong> PSA and Party Alignment successfully capture political dimensions beyond traditional NLP metrics</li>
                    </ol>
                </div>
            </div>
        </div>

        <div class="footer">
            <p>ParliaBench: UK Parliamentary Speech Generation using Large Language Models</p>
            <p style="margin-top: 10px; font-size: 0.9em;">Dataset: ParlaMint-GB v5.0 (2015-2022) | Models: Mistral, Llama, Gemma, Qwen, Yi | QLoRA Fine-Tuning | Comprehensive Evaluation Framework</p>
        </div>
    </div>

    <script>
        function openTab(evt, tabName) {
            // Hide all tab contents
            var tabContents = document.getElementsByClassName("tab-content");
            for (var i = 0; i < tabContents.length; i++) {
                tabContents[i].classList.remove("active");
            }

            // Remove active class from all tab buttons
            var tabButtons = document.getElementsByClassName("tab-button");
            for (var i = 0; i < tabButtons.length; i++) {
                tabButtons[i].classList.remove("active");
            }

            // Show the selected tab content and mark button as active
            document.getElementById(tabName).classList.add("active");
            evt.currentTarget.classList.add("active");

            // Scroll to top of page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }
    </script>
</body>
</html>
