<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ParlaMint-GB Data Processing Pipeline</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            margin-bottom: 40px;
            border-radius: 8px;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        h2 {
            color: #667eea;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }

        h3 {
            color: #764ba2;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        .card {
            background: white;
            padding: 30px;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .stat-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .stat-number {
            font-size: 2em;
            font-weight: bold;
            margin-bottom: 5px;
        }

        .stat-label {
            font-size: 0.9em;
            opacity: 0.9;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: #f5f5f5;
        }

        .pipeline {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .pipeline-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-left: 4px solid #667eea;
            border-radius: 4px;
        }

        .highlight {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #ffc107;
        }

        ul {
            margin-left: 20px;
            margin-top: 10px;
        }

        li {
            margin: 8px 0;
        }

        .footer {
            text-align: center;
            padding: 20px;
            color: #666;
            margin-top: 40px;
        }

        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ParlaMint-GB Data Processing Pipeline</h1>
            <p>UK Parliamentary Proceedings Dataset (2015-2022)</p>
        </header>

        <div class="card" style="background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);">
            <h2 style="color: #333; border-bottom: none; margin-top: 0;">Project Overview</h2>
            <p style="font-size: 1.1em; line-height: 1.8;">
                This project explores political dialogue generation using large language models fine-tuned on UK parliamentary speeches. 
                It encompasses data processing, model selection, fine-tuning with QLoRA, speech generation, and comprehensive evaluation 
                across linguistic, semantic, and political dimensions.
            </p>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-top: 20px;">
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>ðŸ“Š Data Processing</strong><br>
                    <small>447K speeches, 1.9K speakers</small>
                </div>
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>ðŸ¤– Models</strong><br>
                    <small>5 LLMs with QLoRA</small>
                </div>
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>ðŸ’¬ Generation</strong><br>
                    <small>2.7K speeches per model</small>
                </div>
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <strong>ðŸ“ˆ Evaluation</strong><br>
                    <small>12+ metrics across 4 dimensions</small>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>Dataset Overview</h2>
            <p>The ParlaMint-GB dataset version 5.0 from CLARIN contains structured UK parliamentary proceedings with comprehensive metadata including speaker information, political affiliations, gender, and complete speech transcripts.</p>
            
            <div class="stats-grid">
                <div class="stat-box">
                    <div class="stat-number">447,778</div>
                    <div class="stat-label">Total Speeches</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">1,901</div>
                    <div class="stat-label">Unique Speakers</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">11</div>
                    <div class="stat-label">Political Parties</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">~99.94M</div>
                    <div class="stat-label">Total Words</div>
                </div>
            </div>

            <div class="highlight">
                <strong>Time Period:</strong> January 5, 2015 to July 21, 2022<br>
                <strong>Houses:</strong> House of Commons & House of Lords<br>
                <strong>Mean Words per Speech:</strong> 223.2 | <strong>Median:</strong> 99.0
            </div>
        </div>

        <div class="card">
            <h2>Data Cleaning Criteria</h2>
            <ul>
                <li>Kept only parties with more than 1,000 speeches</li>
                <li>Removed speeches with less than 35 words (5th percentile)</li>
                <li>Removed speeches with over 1,580 words (99th percentile)</li>
                <li>Filtered out "Unknown" party affiliation</li>
                <li>Removed "Business of the House" and "Point of Order" sections</li>
                <li>Standardized quotation marks to regular double quotes</li>
            </ul>
        </div>

        <div class="card">
            <h2>Party Distribution</h2>
            <table>
                <thead>
                    <tr>
                        <th>Party</th>
                        <th>Political Orientation</th>
                        <th>Total Speeches</th>
                        <th>Percentage</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Conservative</td>
                        <td>Centre-right</td>
                        <td>263,513</td>
                        <td>58.85%</td>
                    </tr>
                    <tr>
                        <td>Labour</td>
                        <td>Centre-left</td>
                        <td>108,831</td>
                        <td>24.31%</td>
                    </tr>
                    <tr>
                        <td>Scottish National Party</td>
                        <td>Centre-left</td>
                        <td>23,562</td>
                        <td>5.26%</td>
                    </tr>
                    <tr>
                        <td>Liberal Democrats</td>
                        <td>Centre to centre-left</td>
                        <td>23,517</td>
                        <td>5.25%</td>
                    </tr>
                    <tr>
                        <td>Crossbench</td>
                        <td>Unknown</td>
                        <td>11,878</td>
                        <td>2.65%</td>
                    </tr>
                    <tr>
                        <td>Democratic Unionist Party</td>
                        <td>Right</td>
                        <td>6,610</td>
                        <td>1.48%</td>
                    </tr>
                    <tr>
                        <td>Others</td>
                        <td>Various</td>
                        <td>9,867</td>
                        <td>2.20%</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="card">
            <h2>Data Processing Pipeline</h2>
            <div class="pipeline">
                <div class="pipeline-step">
                    <h3>1. XML Parsing & Metadata Extraction</h3>
                    <ul>
                        <li>Parse speaker information from <code>listPerson.xml</code></li>
                        <li>Extract political affiliations with temporal bounds</li>
                        <li>Extract speech content from dated session XML files</li>
                        <li>Filter out procedural elements</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h3>2. Temporal Alignment</h3>
                    <ul>
                        <li>Match speeches to correct political party at time of delivery</li>
                        <li>Handle party changes and role transitions</li>
                        <li>Use temporal validity ranges (<code>@from</code> and <code>@to</code> attributes)</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h3>3. Prompt Extraction</h3>
                    <ul>
                        <li>Identify and separate question prompts from speeches</li>
                        <li>Store prompts as list of strings</li>
                        <li>Clean prompts by removing number and letter prefixes</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h3>4. Political Orientation Classification</h3>
                    <ul>
                        <li>Extract political orientation codes from <code>ParlaMint-listOrg.xml</code></li>
                        <li>Map codes to orientation labels (Left, Centre, Right, etc.)</li>
                        <li>13 distinct orientation categories</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h3>5. Topic Categorization (EuroVoc)</h3>
                    <ul>
                        <li>Direct mapping for 13 categories with clear CAP-EuroVoc correspondence</li>
                        <li>Automated classification using KEVLar for complex categories</li>
                        <li>21 EuroVoc thematic categories</li>
                        <li>Highest confidence score used for final topic assignment</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>EuroVoc Topic Categories</h2>
            <p>Speeches were classified into 21 thematic categories using the EuroVoc taxonomy:</p>
            <div style="columns: 2; column-gap: 30px; margin-top: 20px;">
                <ul>
                    <li>International Relations</li>
                    <li>Law</li>
                    <li>Social Questions</li>
                    <li>Politics</li>
                    <li>Education and Communications</li>
                    <li>Geography</li>
                    <li>Economics</li>
                    <li>Employment and Working Conditions</li>
                    <li>European Union</li>
                    <li>Transport</li>
                    <li>Trade</li>
                    <li>Environment</li>
                    <li>Production, Technology and Research</li>
                    <li>Energy</li>
                    <li>Agriculture, Forestry and Fisheries</li>
                    <li>Finance</li>
                    <li>Industry</li>
                    <li>Business and Competition</li>
                    <li>Agri-foodstuffs</li>
                    <li>International Organisations</li>
                    <li>Science</li>
                </ul>
            </div>
        </div>

        <div class="card">
            <h2>Training Data Structure</h2>
            <p>Each training instance contains the following features:</p>
            <ul>
                <li><strong>speech:</strong> The parliamentary speech text</li>
                <li><strong>section:</strong> Debate section context</li>
                <li><strong>party:</strong> Political party affiliation</li>
                <li><strong>prompts:</strong> Associated question prompts</li>
                <li><strong>house:</strong> House of Commons or House of Lords</li>
                <li><strong>political_orientation_label:</strong> Political orientation classification</li>
                <li><strong>eurovoc_topic:</strong> Thematic category</li>
            </ul>

            <div class="highlight" style="margin-top: 20px;">
                <strong>Train-Test Split:</strong> 80% training / 20% test (random seed: 42)
            </div>
        </div>

        <div class="card">
            <h2>Key Statistics</h2>
            <table>
                <thead>
                    <tr>
                        <th>Statistic</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Mean words per speech</td>
                        <td>223.2</td>
                    </tr>
                    <tr>
                        <td>Median words per speech</td>
                        <td>99.0</td>
                    </tr>
                    <tr>
                        <td>Standard deviation</td>
                        <td>278.7</td>
                    </tr>
                    <tr>
                        <td>Minimum words</td>
                        <td>36</td>
                    </tr>
                    <tr>
                        <td>Maximum words</td>
                        <td>1,579</td>
                    </tr>
                    <tr>
                        <td>25th percentile</td>
                        <td>66</td>
                    </tr>
                    <tr>
                        <td>75th percentile</td>
                        <td>242</td>
                    </tr>
                    <tr>
                        <td>95th percentile</td>
                        <td>872</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="card">
            <h2>Model Selection</h2>
            <p>Five large language models were selected based on their architecture, performance, and compatibility with the UNSLOTH fine-tuning framework. All models use 4-bit quantization for memory efficiency.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Parameters (Quantized)</th>
                        <th>Memory Reduction</th>
                        <th>Inference Speed</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Mistral 7B v0.3</td>
                        <td>3.87B (from 7.25B)</td>
                        <td>62%</td>
                        <td>2.2Ã—</td>
                    </tr>
                    <tr>
                        <td>Llama 3.1 8B</td>
                        <td>4.65B (from 8.3B)</td>
                        <td>58%</td>
                        <td>2.4Ã—</td>
                    </tr>
                    <tr>
                        <td>Gemma 2 9B</td>
                        <td>5.28B (from 9.24B)</td>
                        <td>58%</td>
                        <td>2.2Ã—</td>
                    </tr>
                    <tr>
                        <td>Qwen2 7B</td>
                        <td>~3.8B (4-bit quantized)</td>
                        <td>~60%</td>
                        <td>2.0Ã—+</td>
                    </tr>
                    <tr>
                        <td>Yi 1.5 6B</td>
                        <td>~3.2B (from 6B)</td>
                        <td>~58%</td>
                        <td>2.0Ã—+</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="card">
            <h2>Model Architecture Highlights</h2>
            
            <h3>Mistral 7B v0.3</h3>
            <ul>
                <li><strong>Grouped-Query Attention (GQA):</strong> Groups similar queries for computational efficiency</li>
                <li><strong>Sliding Window Attention (SWA):</strong> Linear complexity for long sequences</li>
                <li><strong>Rotary Position Embedding (RoPE):</strong> Flexible position encoding</li>
            </ul>

            <h3>Llama 3.1 8B</h3>
            <ul>
                <li><strong>Context Window:</strong> Up to 128,000 tokens</li>
                <li><strong>Enhanced Tokenization:</strong> 128,000 token vocabulary</li>
                <li><strong>Multilingual:</strong> Supports 8 languages</li>
                <li><strong>GQA with 8 specialized components</strong></li>
            </ul>

            <h3>Gemma 2 9B</h3>
            <ul>
                <li><strong>42 layers</strong> with interleaved attention mechanism</li>
                <li><strong>Local Sliding Window + Global Attention:</strong> Alternating pattern for comprehensive context</li>
                <li><strong>Logit Soft-Capping:</strong> Stabilizes training and generation</li>
                <li><strong>16 attention heads</strong> with 8 key-value heads</li>
            </ul>

            <h3>Qwen2 7B</h3>
            <ul>
                <li><strong>Dual Chunk Attention:</strong> Partitions long sequences into processable segments</li>
                <li><strong>YARN:</strong> Recalibrates attention weights for longer contexts</li>
                <li><strong>SwiGLU activation functions</strong></li>
                <li><strong>RMSNorm with pre-normalization</strong></li>
            </ul>

            <h3>Yi 1.5 6B</h3>
            <ul>
                <li><strong>Bilingual capabilities:</strong> English and Chinese</li>
                <li><strong>Pre-trained on 3 trillion tokens</strong></li>
                <li><strong>Memory usage:</strong> Reduced from ~12GB to under 4GB</li>
            </ul>
        </div>

        <div class="card">
            <h2>Fine-Tuning Methodology</h2>
            
            <h3>QLoRA (Quantized Low-Rank Adaptation)</h3>
            <p>Parameter-efficient fine-tuning using 4-bit quantization with low-rank matrix adaptation, enabling efficient model customization without massive computational resources.</p>

            <div class="pipeline">
                <div class="pipeline-step">
                    <h3>QLoRA Configuration</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Parameter</th>
                                <th>Value</th>
                                <th>Rationale</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>LoRA Rank (r)</td>
                                <td>16</td>
                                <td>Optimal balance for fast fine-tuning</td>
                            </tr>
                            <tr>
                                <td>LoRA Alpha</td>
                                <td>16</td>
                                <td>Set equal to rank (Î±/r = 1) for baseline</td>
                            </tr>
                            <tr>
                                <td>Target Modules</td>
                                <td>7 layers</td>
                                <td>All linear transformations</td>
                            </tr>
                            <tr>
                                <td>LoRA Dropout</td>
                                <td>0</td>
                                <td>Enable Unsloth optimizations</td>
                            </tr>
                            <tr>
                                <td>Bias Configuration</td>
                                <td>none</td>
                                <td>Faster training, reduced memory</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="pipeline-step">
                    <h3>Training Configuration</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Parameter</th>
                                <th>Value</th>
                                <th>Justification</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Batch Size</td>
                                <td>64</td>
                                <td>GPU memory optimization</td>
                            </tr>
                            <tr>
                                <td>Learning Rate</td>
                                <td>2e-4</td>
                                <td>Standard for LoRA fine-tuning</td>
                            </tr>
                            <tr>
                                <td>Max Steps</td>
                                <td>11,194</td>
                                <td>2 epochs</td>
                            </tr>
                            <tr>
                                <td>Warmup Steps</td>
                                <td>336</td>
                                <td>10% of max steps for stability</td>
                            </tr>
                            <tr>
                                <td>Optimizer</td>
                                <td>AdamW</td>
                                <td>Memory-efficient</td>
                            </tr>
                            <tr>
                                <td>Weight Decay</td>
                                <td>0.01</td>
                                <td>Prevents overfitting on political data</td>
                            </tr>
                            <tr>
                                <td>Max Sequence Length</td>
                                <td>1024</td>
                                <td>Optimal for dataset median length</td>
                            </tr>
                            <tr>
                                <td>Scheduler</td>
                                <td>Linear</td>
                                <td>Linear learning rate schedule</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="pipeline-step">
                    <h3>System Prompt</h3>
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 4px; font-family: monospace; margin-top: 10px;">
                        You are a seasoned UK parliamentary member. Use proper British parliamentary language appropriate for the specified House. The speech should reflect the political orientation and typical positions of the specified party on the given topic.
                    </div>
                </div>

                <div class="pipeline-step">
                    <h3>Context Fields (Input to Model)</h3>
                    <ul>
                        <li><strong>PARTY:</strong> Political party affiliation (e.g., Conservative)</li>
                        <li><strong>EUROVOC TOPIC:</strong> Thematic classification (e.g., TRADE)</li>
                        <li><strong>SECTION:</strong> Parliamentary debate section</li>
                        <li><strong>POLITICAL ORIENTATION:</strong> Orientation label (e.g., Centre-right)</li>
                        <li><strong>HOUSE:</strong> House of Commons or House of Lords</li>
                        <li><strong>INSTRUCTION:</strong> Question prompt or generic instruction</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>Tools & Environment</h2>
            <div class="stats-grid">
                <div class="stat-box">
                    <div class="stat-label">Framework</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">Unsloth + HuggingFace</div>
                </div>
                <div class="stat-box">
                    <div class="stat-label">Backend</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">PyTorch</div>
                </div>
                <div class="stat-box">
                    <div class="stat-label">Hardware</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">AWS A100 GPU</div>
                </div>
                <div class="stat-box">
                    <div class="stat-label">Training Method</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">Supervised Fine-Tuning</div>
                </div>
            </div>

            <h3 style="margin-top: 30px;">Key Libraries</h3>
            <ul>
                <li><strong>Hugging Face Transformers:</strong> Model loading, tokenization, and training</li>
                <li><strong>Unsloth:</strong> Memory-efficient LoRA-based fine-tuning</li>
                <li><strong>PyTorch:</strong> Tensor operations and GPU acceleration</li>
                <li><strong>Datasets:</strong> Corpus management and preprocessing</li>
                <li><strong>scikit-learn:</strong> Train-test splitting and preprocessing</li>
            </ul>
        </div>

        <div class="card">
            <h2>Output Structure</h2>
            <p>Each trained model produces LoRA adapter weights (not full models) saved in the following format:</p>
            <ul>
                <li><code>adapter_model.bin</code> - LoRA adapter weights</li>
                <li><code>adapter_config.json</code> - Configuration (rank, alpha, target modules)</li>
                <li><code>tokenizer.json</code> - Tokenizer files</li>
                <li><code>tokenizer_config.json</code> - Tokenizer configuration</li>
                <li><code>special_tokens_map.json</code> - Special token mappings</li>
            </ul>
            <p style="margin-top: 15px;"><strong>Storage location:</strong> <code>./trained_models/MODEL_NAME/</code></p>
        </div>

        <div class="card">
            <h2>Speech Generation Pipeline</h2>
            <p>A systematic speech generation system that loads trained models and creates political speeches based on structured inputs.</p>

            <h3>Input Distribution</h3>
            <p>All models received identical generation tasks with realistic distributional characteristics:</p>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                <div>
                    <h4>House Distribution</h4>
                    <ul>
                        <li>House of Commons: 78%</li>
                        <li>House of Lords: 22%</li>
                    </ul>
                </div>
                <div>
                    <h4>Top Parties by Weight</h4>
                    <ul>
                        <li>Conservative: 59%</li>
                        <li>Labour: 24%</li>
                        <li>Scottish National Party: 5%</li>
                        <li>Liberal Democrats: 5%</li>
                    </ul>
                </div>
            </div>

            <h3>Generation Parameters</h3>
            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Value</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Speeches per Model</td>
                        <td>2,700</td>
                        <td>Comprehensive evaluation dataset</td>
                    </tr>
                    <tr>
                        <td>Temperature</td>
                        <td>0.7</td>
                        <td>Balances coherence and variation</td>
                    </tr>
                    <tr>
                        <td>Top-p (Nucleus Sampling)</td>
                        <td>0.85</td>
                        <td>Focused yet diverse outputs</td>
                    </tr>
                    <tr>
                        <td>Repetition Penalty</td>
                        <td>1.2</td>
                        <td>Prevents redundant phrasing</td>
                    </tr>
                    <tr>
                        <td>Batch Size</td>
                        <td>32</td>
                        <td>3Ã— speed improvement</td>
                    </tr>
                    <tr>
                        <td>Min Word Count</td>
                        <td>43</td>
                        <td>P10 threshold for quality</td>
                    </tr>
                    <tr>
                        <td>Max Word Count</td>
                        <td>635</td>
                        <td>P90 threshold for quality</td>
                    </tr>
                    <tr>
                        <td>Max New Tokens</td>
                        <td>850</td>
                        <td>1.33Ã— P90 speech length</td>
                    </tr>
                </tbody>
            </table>

            <div class="highlight" style="margin-top: 20px;">
                <strong>Decoding Strategy:</strong> Nucleus sampling (top-p) chosen over greedy/beam search to avoid repetitive or incoherent text
            </div>
        </div>

        <div class="card">
            <h2>Speech Validation Process</h2>
            <p>9-step validation procedure to ensure quality, coherence, and relevance of generated speeches:</p>
            
            <div class="pipeline">
                <div class="pipeline-step">
                    <strong>1. Template Marker Detection</strong><br>
                    Detects 27 template artifacts (role markers, context labels, special tokens)
                </div>
                <div class="pipeline-step">
                    <strong>2. Unicode Corruption Detection</strong><br>
                    Identifies 14 corruption patterns and checks 11 forbidden Unicode ranges (CJK, Cyrillic, Arabic, etc.)
                </div>
                <div class="pipeline-step">
                    <strong>3. Language Detection</strong><br>
                    Uses spacy-langdetect to flag non-English text (&gt;85% confidence threshold)
                </div>
                <div class="pipeline-step">
                    <strong>4. Repetition Detection</strong><br>
                    Three patterns: (1) Same word &gt;3 times, (2) Sequences of 3-7 words &gt;3 times, (3) Counting patterns
                </div>
                <div class="pipeline-step">
                    <strong>5. Semantic Relevance Check</strong><br>
                    Cosine similarity between speech and context (threshold: &lt;0.08 flagged as off-topic)
                </div>
                <div class="pipeline-step">
                    <strong>6. Length Constraints</strong><br>
                    Validates word count (43-635 words)
                </div>
                <div class="pipeline-step">
                    <strong>7. Concatenation Detection</strong><br>
                    Detects multiple opening phrases (â‰¥4 instances of "My Lords", "Mr Speaker", etc.)
                </div>
                <div class="pipeline-step">
                    <strong>8. Corrupted Endings Detection</strong><br>
                    Identifies nonsensical endings
                </div>
                <div class="pipeline-step">
                    <strong>9. Refusal Pattern Matching</strong><br>
                    Catches AI refusal patterns ("I cannot generate", "I'm sorry but...")
                </div>
            </div>
        </div>

        <div class="card">
            <h2>Evaluation Framework</h2>
            <p>Comprehensive multi-dimensional assessment system evaluating linguistic quality, semantic coherence, political alignment, and overall effectiveness.</p>

            <h3>1. Linguistic Quality & Diversity Metrics</h3>
            
            <div class="pipeline">
                <div class="pipeline-step">
                    <h4>Perplexity</h4>
                    <p>Measures how natural the text appears to a language model. Lower scores indicate more human-like, predictable text.</p>
                    <ul>
                        <li><strong>Model:</strong> GPT-2 base (117M parameters)</li>
                        <li><strong>Processing:</strong> Max 512 words per speech, batch size 8</li>
                        <li><strong>Interpretation:</strong> Lower = more natural</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h4>Distinct-N (N=1,2,3,4)</h4>
                    <p>Evaluates lexical diversity by measuring ratio of unique n-grams to total tokens.</p>
                    <ul>
                        <li><strong>Distinct-1:</strong> Unique unigrams (basic lexical diversity)</li>
                        <li><strong>Distinct-2:</strong> Unique bigrams (phrase-level variety)</li>
                        <li><strong>Distinct-3/4:</strong> Multi-word patterns (sophisticated language use)</li>
                        <li><strong>Interpretation:</strong> Higher = more diverse vocabulary</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h4>Self-BLEU</h4>
                    <p>Measures similarity between generated texts from the same model to detect repetitive content.</p>
                    <ul>
                        <li><strong>Method:</strong> Each speech compared to all others from same model</li>
                        <li><strong>Interpretation:</strong> Lower = higher diversity (desirable)</li>
                    </ul>
                </div>
            </div>

            <h3>2. Semantic Coherence & Text Quality</h3>
            
            <div class="pipeline">
                <div class="pipeline-step">
                    <h4>GRUEN Score</h4>
                    <p>Comprehensive quality metric combining Grammaticality, non-Redundancy, focUs, structurE, and coNherence.</p>
                    <ul>
                        <li><strong>Grammaticality:</strong> BERT perplexity + CoLA classifier (0-1 scale)</li>
                        <li><strong>Non-Redundancy:</strong> LCS, Edit Distance, Word Overlap between sentences</li>
                        <li><strong>Focus:</strong> Word Mover's Distance + SpaCy semantic similarity</li>
                        <li><strong>Formula:</strong> GRUEN = min(1, max(0, G + R_penalty + F_penalty))</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h4>BERTScore</h4>
                    <p>Measures semantic similarity between generated and real speeches using contextualized embeddings.</p>
                    <ul>
                        <li><strong>Model:</strong> RoBERTa-large (auto-selected for English)</li>
                        <li><strong>References:</strong> N=6 real speeches from ParlaMint-GB</li>
                        <li><strong>Metrics:</strong> Precision, Recall, F1</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h4>MoverScore</h4>
                    <p>Computes optimal transport cost between embedding distributions using Earth Mover's Distance.</p>
                    <ul>
                        <li><strong>Model:</strong> DistilBERT-base-uncased</li>
                        <li><strong>Method:</strong> IDF-weighted embeddings with POT library</li>
                        <li><strong>References:</strong> N=6 most semantically similar speeches</li>
                        <li><strong>Score Range:</strong> 0-1 (higher = better alignment)</li>
                    </ul>
                </div>
            </div>

            <h3>3. Political Alignment Metrics</h3>
            
            <div class="pipeline">
                <div class="pipeline-step">
                    <h4>Political Spectrum Alignment (PSA)</h4>
                    <p>Measures ideological alignment with expected political orientation (13-point scale from Far-left to Far-right).</p>
                    <ul>
                        <li><strong>Model:</strong> all-mpnet-base-v2 sentence transformer</li>
                        <li><strong>Method:</strong> Cosine similarity to orientation centroids</li>
                        <li><strong>Formula:</strong> PSA = cosine_similarity Ã— max(0, 100 - d/12 Ã— 100)</li>
                        <li><strong>Range:</strong> 0-1 (higher = better alignment)</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <h4>Party Alignment</h4>
                    <p>Assesses whether speech captures party-specific linguistic characteristics.</p>
                    <ul>
                        <li><strong>Model:</strong> all-mpnet-base-v2 sentence transformer</li>
                        <li><strong>Method:</strong> Cosine similarity to party-specific centroids</li>
                        <li><strong>Range:</strong> 0-1 (higher = better party alignment)</li>
                    </ul>
                </div>
            </div>

            <h3>4. LLM-as-a-Judge Evaluation</h3>
            <p>Automated assessment using Flow-Judge-v0.1 (3.8B parameters, 4-bit quantization) across six dimensions on a 10-point scale:</p>

            <table style="margin-top: 20px;">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Evaluation Criteria</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Coherence</strong></td>
                        <td>Logical flow, argument connectivity, parliamentary structure</td>
                    </tr>
                    <tr>
                        <td><strong>Conciseness</strong></td>
                        <td>Efficient message delivery without excessive verbosity (parliamentary context)</td>
                    </tr>
                    <tr>
                        <td><strong>Relevance</strong></td>
                        <td>Direct addressing of prompt/question with complete coverage</td>
                    </tr>
                    <tr>
                        <td><strong>Authenticity</strong></td>
                        <td>Natural Westminster discourse vs AI-generated patterns</td>
                    </tr>
                    <tr>
                        <td><strong>Political Appropriateness</strong></td>
                        <td>Alignment with party's typical positions and rhetoric</td>
                    </tr>
                    <tr>
                        <td><strong>Overall Quality</strong></td>
                        <td>Effectiveness as political communication, argumentation sophistication</td>
                    </tr>
                </tbody>
            </table>

            <div class="highlight" style="margin-top: 20px;">
                <strong>Configuration:</strong> Batch size 32, Temperature 0.3, Max new tokens 2000, Default score -1 for errors
            </div>
        </div>

        <div class="card">
            <h2>Evaluation Summary</h2>
            <div class="stats-grid">
                <div class="stat-box">
                    <div class="stat-label">Linguistic Metrics</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">Perplexity, Distinct-N, Self-BLEU</div>
                </div>
                <div class="stat-box">
                    <div class="stat-label">Semantic Metrics</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">GRUEN, BERTScore, MoverScore</div>
                </div>
                <div class="stat-box">
                    <div class="stat-label">Political Metrics</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">PSA, Party Alignment</div>
                </div>
                <div class="stat-box">
                    <div class="stat-label">Judge Metrics</div>
                    <div style="font-size: 1.2em; margin-top: 10px;">6 Dimensions (1-10 scale)</div>
                </div>
            </div>
        </div>

        <div class="footer">
            <p>UK Parliamentary Speech Generation using Large Language Models</p>
            <p style="margin-top: 10px; font-size: 0.9em;">Dataset: ParlaMint-GB v5.0 (2015-2022) | Models: Mistral, Llama, Gemma, Qwen, Yi | QLoRA Fine-Tuning | Comprehensive Evaluation Framework</p>
        </div>
    </div>
</body>
</html>
